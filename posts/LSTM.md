---
title: LSTM（长短期记忆）知识梳理
date: 2025-10-24
updated: 2025-10-24
category: "DeepLearning"
tags: ["LSTM"]
excerpt: "本文梳理LSTM研究所需的基础知识"
---

> 我们在学习LSTM的概念之前，需要先理解RNN（循环神经网路），因为LSTM是RNN的一个优化模型，具体来说，LSTM是为了解决标准 RNN 在处理长序列时的**梯度消失/爆炸问题**而设计的

## RNN

### 记忆机制

从先前的输入中获取信息，用于影响当前的输入和输出。

RNN循环神经网络 区别于 传统前馈神经网络（MLP,CNN）的关键就在于记忆机制。为什么需要记忆？我们在处理序列数据（如文本、语音、时间序列）的时候，**当前时刻的输出依赖于历史信息**，RNN通过引入**隐藏状态**（hidden state），实现了对过去信息的记忆，从而建模序列中的时间依赖关系。

#### 隐藏状态（hidden state）

RNN 在每个时间步 $t$ 接收两个输入：

- 当前时刻的输入 $x_t$
- 上一时刻的隐藏状态 $h_{t-1}$

然后计算当前时刻的隐藏状态 $h_t$，并将其传递给下一时刻：

$$
h_t = \tanh(W_h h_{t-1} + W_x x_t + b_h)
$$

其中：

- $W_h$：隐藏状态到隐藏状态的权重矩阵（**记忆的延续**）
- $W_x$：输入到隐藏状态的权重矩阵
- $b$：偏置项
- $\tanh$：激活函数（也可用 ReLU 等，但 $\tanh$ 更常见）

**这个 $h_t$ 就是 RNN 的“记忆”载体**。它编码了从时间步 1 到 $t$ 的所有输入信息（理想情况下）。

展开公式：
$$
\begin{aligned}
h_t &= f(W_h h_{t-1} + W_x x_t) \\\\
&= f(W_h (f(W_h h_{t-2} + W_x x_{t-1})) + W_x x_t) \\\\
&\approx f(W_h^t h_0 + \sum_{k=1}^{t} W_h^{t-k} W_x x_k)
\end{aligned}
$$

从这个式子可以看出：

- 当前隐状态 $h_t$ 是所有过去输入 $x_1, x_2, ..., x_t$ 的加权组合；
- 权重随时间衰减（因为 $W_h^{t-k}$ 的幂次会让远处信息衰减）。

所以理论上RNN能记住很久以前的东西，但是在实验中，越久远的东西越容易消失。这就引出了——**梯度消失与梯度爆炸问题**

## LSTM

### 细胞状态 Cell state

细胞状态是LSTM 中的**长时间记忆**，它贯穿整个序列数据流，可以在长时间内保留信息而不被遗忘。细胞状态通过遗忘门和输入门的调控，选择性地保留或丢弃信息，确保有用的信息可以长时间保留。

### 三大门控机制

#### 1. 遗忘门 Forget Gate

决定丢弃哪些旧记忆。

#### 2. 输入门 Input Gate

决定更新哪些新信息。

#### 3. 输出门 Output Gate

决定当前输出什么。